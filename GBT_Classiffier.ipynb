{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/raj/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "\n",
    "def GBT_classifier():\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"Gbt_classifier\").getOrCreate() \n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "    data = spark.read.format(\"csv\")\\\n",
    "      .option(\"header\", \"true\")\\\n",
    "      .option(\"inferSchema\", \"true\")\\\n",
    "      .load(\"/home/raj/Downloads/notenook/adult2.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    data.show()\n",
    "\n",
    "    categoricalColumns = [\"workclass\", \"occupation\"]\n",
    "\n",
    "    stages = []\n",
    "    for categoricalCol in categoricalColumns:\n",
    "        stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "\t#.fit(ad_data)\n",
    "\t#df_numeric = stringIndexer.transform(ad_data)\n",
    "\t#df_numeric.repartition(1).repartition(1).write.csv('indexer')\n",
    "\t#print df_numeric.select('workclass','workclassIndex').show(5)\n",
    "\t#In the above line for example, it takes workclass string and concatinates with the address(\"Index\")\n",
    "        encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n",
    "\t#print encoder.outputCol\n",
    "        stages += [stringIndexer, encoder]\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "    label_stringIdx = StringIndexer(inputCol = \"income\", outputCol = \"labelindex\")\n",
    "\n",
    "    stages += [label_stringIdx]\n",
    "\n",
    "    numericCols = [\"age\", \"hours_per_week\"]\n",
    "\n",
    "    assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) \n",
    "\n",
    "    assemblerInputs=list(assemblerInputs) + numericCols\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "    stages += [assembler]\n",
    "\n",
    "# Run the feature transformations.\n",
    "#  - fit() computes feature statistics as needed.\n",
    "#  - transform() actually transforms the features.\n",
    "\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "\n",
    "    pipelineModel = pipeline.fit(data)\n",
    "    dataset = pipelineModel.transform(data)\n",
    "\n",
    "    cols = data.columns\n",
    "\n",
    "    selectedcols = [\"labelindex\", \"features\"] + cols\n",
    "    dataset = dataset.select(selectedcols)\n",
    "\n",
    "\n",
    "\n",
    "    dataset.printSchema()\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "\n",
    "    featureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(dataset)\n",
    "    print(featureIndexer)\n",
    "\n",
    "\n",
    "\n",
    "    featuredf=featureIndexer.transform(dataset)\n",
    "\n",
    "    featuredf.show()\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "    (trainingData, testData) = featuredf.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "    trainingData.show()\n",
    "\n",
    "    testData.show()\n",
    "\n",
    "# Train a GBT model.\n",
    "    gbt =GBTClassifier(labelCol=\"labelindex\", featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "    pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "    model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    predictions.printSchema()\n",
    "\n",
    "    trainingData.show()\n",
    "\n",
    "# Select example rows to display.\n",
    "    predictions.select(\"prediction\", \"labelindex\", \"features\",\"probability\",\"rawPrediction\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"labelindex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "    print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "    gbtModel = model.stages[1]\n",
    "\n",
    "    # summary only\n",
    "    return gbtModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GBT_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
